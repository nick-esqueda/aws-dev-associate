---
S3 SECURITY

s3 is private by default

bucket policy = type of resource policy 
resource policy = like an IAM identity policy, but attached to resources
- (inverse of each other)
- identity policies define what that identity can access
- resource policies define who can access that resource

only 1 bucket policy per bucket (but can have many policy statements)

huge benefit of resource policies:
- you can allow identities from ANY account to access the resource
- note: you can't create identity policies for IAM users outside of an account.
  - so, you CAN'T grant users outside the account access to an S3 bucket thru ID policies
- resource policies can specify any user in ANY account and grant/deny access to them
- can grant anonymous access (don't know who the user/principal is) 
  - IAM policies can't do this - you can't attach an ID policy to 'nothing'

bucket policies have a "Principal" key in the Statement
- the principal could be IAM users, AWS accounts, anonymous (unauth.nd to AWS), *, etc.
- ID policies don't have a Principal property bc it's implied that the principal is the identity using that policy

can block IP addresses in resource/bucket policies

remember: policies get merged.
- if IAM user trying to access bucket, that user's policy + the bucket policy apply
  - (same for IAM users outside the bucket acct - the user's + bucket's policy apply)
- anonymous principles are only restricted by the bucket policy
  - (bc logically there is no identity policy for anonymous principles)

ACLs
- legacy - AWS suggests to use bucket policies/ID policies instead
  - ACLs are inflexible & have simple perm.s only
  - can kind of ignore they exist
  - never use ACLs unless you need to
- ACLs are a subresource of a bucket

Block Public Access feature
- added to S3 due to PR disasters - lots of data leaks bc users didn't understand S3 security properly - lot of user misconfigurations
- another security layer in front of the resource policy
  - "block public access" apply no matter what the bucket policy says
- applies only to anonymous users, not IAM identities
- tip: if you grant public access in a bucket policy but it's not working, it's probably because "block public access" is enabled

tradeoffs between identity policies vs resource policies - choice depends on use case
- res. policies - good for base level permissions on svc

---
S3 STATIC HOSTING

allows access via HTTP (using web browsers)

must set an index and error document
- must be html docs bc that's what static hosting delivers

"website endpoint" = url to hit the bucket from the internet (DNS)
if you want a custom domain, must make bucket name same as domain name

use cases:
offloading:
- serve static images S3 instead of serving them from a compute service (ec2)
  - this will save costs since compute is expensive - s3 isn't
  - ex: dynamic html doc retrieved from ec2 points to images in s3 bucket
out-of-band pages:
- if main server (ec2) is offline for maintenance, can route customers to s3 to get a maintenance page
  - using another ec2 instance to serve the maintenance page might be an issue, because what if ec2 itself is having issues? better to use s3 scenarios like this[

pricing:
1. storing data in s3 - GB/month
2. transferring data out of buckets - per GB charge
- never charged for transferring in to buckets
3. data requests (operations) - per 1000 requests
- concern for static hosting if lot of customers
free tier:
- 5 GB storage

---
OBJECT VERSIONING & MFA DELETE

versioning is disabled by default
! once turned on, you CAN'T turn it off
- you can suspend it, and re-enable, but can't turn off
  - suspending: "This suspends the creation of object versions for all operations but preserves any existing object versions.
! you're billed for ALL versions of ALL objs
- only way to 0 the costs is to delete the bucket & reupload to new bucket w/o vers.ng
1. disabled (-> enabled)
2. enabled (-> suspended)
3. suspended (-> enabled)

objects in s3 buckets have an "id" property
if vers.ng disabled, "id" = "null" for all objs
once enabled, objs will get an id
when modifying an obj, new copy with same key ("name") is created with a new ID, orig. copy is retained 
you can get certain versions by specifying the ID in the request
if no ID in req, the latest/current version is returned by default

DELETING W/VERSIONING
w/o ID specified:
- s3 creates a "delete marker" which basically hides all versions of the obj
  - all obj versions are still retained!
  - QUOTE FROM AWS CONSOLE:
    - "Deleting the specified objects adds delete markers to them"
    - "If you need to undo the delete action, you can delete the delete markers"
- you can delete the "delete marker", which essentially "un-deletes" the obj
with ID specified:
- s3 will actually delete the obj version with specified ID
- most recent version becomes the curr/latest version
  - if deleting a vers. other than the current vers., the curr vers. stays same (nothing happens besides the selected vers. being deleted)
- NOTE: everything is "permanent" when vers.ng is enabled
  - deleting a "delete marker" is PERMANENT

MFA DELETE
a config for the obj versioning feature
if enabled, MFA device is required to:
- delete obj versions
- change state of mfa delete (i.e. suspending or re-enabling)
HOW: when calling API, must provide MFA device's serial num + MFA code


---
S3 PERFORMANCE OPTIMIZATION

"Single PUT Upload"
- default for s3:PutObject uses single data stream to upload to s3
- (typical downloads from a browser use multiple streams)
- 1 data stream = slower than multiple data streams
- if network fails after 4.5 of 5.0 GB are uploaded, then the WHOLE upload fails
- AWS has max 5GB object upload to S3 using single PUT upload
  - ... but you shouldn't trust it for anything near 5GB...
- bad idea for unreliable connections that need to upload to s3

"Multipart Upload"
- faster
- data is broken up into multiple parts for upload
! min data size for multipart is 100MB
  - you can't use multipart upload if blob is under 100MB
- no reason not to use it if >100MB - benefits are to great to not use it
- can have 10,000 MAX parts - each range from 5MB -> 5GB (last part can be smaller than 5MB)
- each part can fail & be restarted in isolation, instead of restarting the whole thing
- means much less risk for uploading large data to s3
- parts upload in parallel. so tranfer rate is increased 

S3 TRANSFER ACCELERATION
- data enters the closest/best performing AWS edge location
- from there, it enters the AWS global network
  - AWS global network:
    - used to connect regions together
    - optimized for efficiency
    - public internet might take strange paths - AWS network takes direct paths
- benefits of transfer accel. increase as data size or distance increases
- turned off by default
- bucket name must be DNS compatible
  - no periods in bucket name
- when enabling, you get a DNS endpoint for the faster transfers
  - have to use that URL for accel.d transfers
  - resolved to an edge location closest to you

---
KEY MANAGEMENT SERVICE (KMS)

used by many AWS svcs

regional & public service (AWS public zone)

create, store, manage keys
operates with both symmetric and asymmetric keys
does encrypt & decrypt (and others...)
! keys never leave KMS - keys are meant to be locked inside

! provides FIPS 140-2 (*L2) standard
- some features have L3 compliance
- "KMS is a secure and resilient service that uses FIPS 140-2 validated hardware security modules to isolate and protect your keys"

some KMS keys are called CMKs (Customer Master Keys) - old naming convention

keys can be used by you, applications, & aws svcs

"KMS Keys" are just containers for the physical real keys
- has below fields:
  - ID
  - date
  - policy
  - desc
  - state

can generate or import keys

keys themselves are encrypted when stored in KMS
keys are decrypted before they encrypting/decrypting data

on encrypt request, need to send data + key you want to use
on decrypt request, do NOT need to specify the key to use
- the key info is encoded in the cipher text. KMS can tell which one to use

permissions:
- encrypt, decrypt, key creation, etc. are all SEPARATE permissions
  - called "role separation"
  - key administrator has diff perm.s than IAM users that use the key
    - key admins can delete the key

KMS keys can encrypt MAX 4KB of data
KMS can generate DEKs (Data Encryption Keys) using KMS keys - GenerateDataKey operation
DEKs can encrpyt/decrypt data >4KB
DEKs are linked to the KMS key that created it
- the DEK knows which key created it
KMS gives DEKs to the client - YOU or the SERVICE uses the DEK
- DEKs are NOT stored in KMS

DEK architecture
1. user requests DEK from KMS
2. DEK is created (using a KMS key) (GenerateDataKey)
3. KMS gives user the plaintext DEK and the encrypted DEK
  - DEK is encrypted using the same KMS key that generated it
4. user encrypts their data (ex. s3 object) with the plaintext DEK
5. user discards the plaintext DEK
6. user stores the encrypted DEK with the newly encrypted data
when need to decrypt data...
7. user requests for DEK decryption - send encrypted DEK to KMS
8. KMS decrypts the DEK using the same KMS key used to generate it
9. user uses decrypted DEK to decrypt the data
10. user discards the decrypted DEK

you can encrypt many objects/data with 1 DEK, or use a new DEK for each object/data

default = KMS keys are isolated in a region - CANNOT move a key to different region
- can't extract a key from the KMS service either
- KMS aliases are also regional
multi-region keys can be used (key replicated across regions)
- relatively new feature

customer owned keys:
- type 1: AWS managed - created automatically when using a svc that integrates w/KMS
- type 2: customer managed - created explicitly by customer for application/AWS svc usage

all keys can be rotated 
- off by default for cust. managed keys
- on by default for AWS managed keys (you can't turn off)
- done yearly when turned on
backing keys are kept so old data can still be decrypted

every key has a Key Policy (resource policy - like S3 bucket policies)
you must explicitly configure KMS to trust the account that the key is created in via key policy
- this trust isn't automatic, like it is with resources in other svcs
- need to do this before granting access to a key via identity policies
- kinda like the "allow all public access" feature in S3 - gotta do it twice
- ex. identity policy usually allows encrypt/decrypt for all KMS keys, while the key policy specifies what exact principals are allowed to encrypt, decrypt, create, etc.

Key Policy from demo
{
    "Id": "key-consolepolicy-3",
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Enable IAM User Permissions",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::711387134335:root"
            },
            "Action": "kms:*",
            "Resource": "*"
        },
        {
            "Sid": "Allow access for Key Administrators",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::711387134335:user/iamadmin"
            },
            "Action": [
                "kms:Create*",
                "kms:Describe*",
                "kms:Enable*",
                "kms:List*",
                "kms:Put*",
                "kms:Update*",
                "kms:Revoke*",
                "kms:Disable*",
                "kms:Get*",
                "kms:Delete*",
                "kms:TagResource",
                "kms:UntagResource",
                "kms:ScheduleKeyDeletion",
                "kms:CancelKeyDeletion",
                "kms:RotateKeyOnDemand"
            ],
            "Resource": "*"
        },
        {
            "Sid": "Allow use of the key",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::711387134335:user/iamadmin"
            },
            "Action": [
                "kms:Encrypt",
                "kms:Decrypt",
                "kms:ReEncrypt*",
                "kms:GenerateDataKey*",
                "kms:DescribeKey"
            ],
            "Resource": "*"
        },
        {
            "Sid": "Allow attachment of persistent resources",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::711387134335:user/iamadmin"
            },
            "Action": [
                "kms:CreateGrant",
                "kms:ListGrants",
                "kms:RevokeGrant"
            ],
            "Resource": "*",
            "Condition": {
                "Bool": {
                    "kms:GrantIsForAWSResource": "true"
                }
            }
        }
    ]
}

---
S3 OBJECT ENCRYPTION CSE/SSE

! buckets are not encrypted! objects are.
- define encryption at the object level
- ("bucket default encryption" is a different feature, separate)
- each obj could use diff encryption settings

S3 encryption (CSE/SSE) refers to encryption AT REST. 
(data is always encrypted in transit, i.e. HTTPS)

CSE = Client Side Encryption
  - the client encrypts the data before even uploading to S3
  - client takes all risk and control over encryption
  - you manage the key, and which key for which obj
  - not related to S3 at all, technically could do both CSE & S3 SSE
SSE = Server Side Encryption
  - client sends plaintext. S3 encrypts the plaintext data.

SSE/encryption at rest is mandatory in S3
- you can only change what type is used

2 Components to SSE
1. encryption & decryption (symmetric) processes
2. actual mgmt of keys

each type of SSE handles the above 2 components differently
your choice depends on how much you trust S3
your choice impacts permissions
multiple types of SSE
- SSE-C = SSE with Customer-Provided Keys
  - client generates/manages key, but compute for encyption/decryption is offloaded to S3
  1. client provides key and plaintext obj
  2. S3 encrypts the obj
  3. S3 makes a hash of the key
  4. S3 discards the key
  5. S3 stores the hash along with the ciphertext object
  - you trust that S3 will discard your key and not hold on to it
- SSE-S3 = SSE with AWS S3 Managed Keys
  - the default method 
  - uses AES256 (sometimes SSE-S3 is referred to as AES256)
  - S3 handles both encryption process, AND key generation/mgmt
  - unique key for each obj
  1. client just provides platintext obj
  2. S3 generates an object key
  3. S3 encrypts the obj with obj key
  4. S3 encrypts the obj key with the S3 Key
    - S3 Key is totally invisble to customers, you can't do anything to/with it.
  5. S3 discards the plaintext obj key
  6. S3 stores the encrypted obj key and ciphertext obj
  - you don't have control over keys here
    - shouldn't use this if compliance requires full control (key access, rotation, etc.)
  - no role separation
    - can't restrict full admin users from seeing objs
- SSE-KMS = SSE with KMS Keys Stored in KMS
  - you create KMS key so you control it, & own permissions on the key
    - OR, you can use an S3 default key. (can't change key policy, set rotations, etc.)
      - customer managed vs aws managed key
      - ! don't confuse this with SSE-S3
  - S3 & AWS do key mgmt, S3 does encryption process
  - S3 requests a DEK (using the KMS key) from KMS for each obj
  - follows DEK architecture (check earlier notes)
  - good option bc you have full control over the key/key mgmt
    - logging/auditing, rotation, role separation, etc.
  - can restrict full admins from seeing objects by not giving permissions to the KMS key

---
S3 BUCKET KEYS

used for improving KMS scalability (and S3 scalability in turn)

before bucket keys (if using sse-kms):
  each s3 Put causes a call to KMS to make a unique DEK (using the 1 specified KMS Key)
  - scaled poorly - 1-to-1 for each s3 upload 
  KMS calls have costs and throttling limits
  - 5.5k, 10k, or 50k per second across ALL regions 
  using a single KMS key results in a scaling limit for PUTS per second per key
after bucket keys:
  KMS gives a "bucket key" to the S3 bucket. that bucket key is used to make DEKs
  - the bucket key is TIME LIMITED - new bucket key will be used after time limit
  - bucket key is generated from the original KMS key
  this offloads a lot of the work from KMS to S3 (reduces KMS API calls)
  not retroactive - only affects objs after it's enabled on the bucket

NOTE: KMS CloudTrail events would show the bucket ARN instead of obj ARN
- b/c KMS is creating a bucket key, not a DEK for an object
fewer cloudtrail events - b/c KMS used less often, just to make bucket keys

works with S3 data replication across regions
encryption settings for the obj would also be same for replicated one
- but, if replicating plaintext obj to a bucket using bucket keys, the obj gets encrypted once replicated to the destination

---
S3 STORAGE CLASSES

each class offers different storage features/specs

S3 STANDARD
! Use S3 Standard for frequently accessed data that is important an non replaceable.
- objs get replicated across AT LEAST 3 AZs in the region
- objs can be public
- offers 9 9's of durability
- content-MD5 checksums
- CRCs (Cyclic Redundancy Checks) used to detect/fix data corruption
- billing:
  - GB/month fee for data storaged
  - $ per GB charge for transfer OUT
  - no fee for transfer IN data
  - price per 1K requests
  - no min duration, no min obj size
! if you get 200 OK from S3 API, then you know it was stored durability.

S3 Standard-IA (Infrequent Access)
! useful for long-lived data, which is important, but accessed infrequently
- same as S3 except below...
- about 1/2 cost to store data
- billing:
  - per request charge - same as s3 standard
  - data transfer out - same as s3 standard
  ! retrieval fee (in addition to transfer out fee!) - per GB data
  - min duration charge of 30 days (can store objs for 2 days, but get chg.d for 30)
  - min capacity charge of 128KB per obj
    - not useful for lots of tiny objects

S3 One Zone-IA (Infrequnt Access)
! For long-lived, non-critical, replaceable data that isn't accessed frequently
- same as S3 Standard IA except below...
- data is only stored in ONE AZ
  - cheaper storaged for greater risk of AZ failure
- still 9 9's of durability (ASSUMING THE AZ DOESN'T FAIL)
  - data still replicated, but only within the one AZ

S3 Glacier - Instant (Instant Retrieval)
! used for long-lived data, accessed ~1/qtr, with very fast access
- like S3 Standard-IA, but cheaper storage, more expensive retrieval, longer min.s
  - want data very infrequently, but want it fast (millisecs) once it's time
- min duration charge of 90 days
- costs more if you need to access the data, but less if you don't

S3 Glacier - Flexible Retrieval (Formerly just S3 Glacier)
! For archival data where realtime or frequent access isn't needed. mins-hrs retrieval
- 1/6 the cost of data storage compared to S3 Standard
- same durability/redundancy/replication as S3 Standard
- "cold objects"
  - can't be made public
  - you can see them in console, but those are just ptrs to the objs
- have to do a retrieval process to get objects
  - job that S3 has to run
  - once retrieved, they're stored with S3 Standard class on temp basis
  - you pay for the retrieval process
- retrieval options:
  - expedited (1-5 mins)
  - standard (3-5 hours)
  - bulk (5-12 hours)
  - faster = more $$$
! first byte latency = mins or hours
- 40 KB min billing size
- 90 day min duration billing

S3 GLACIER - DEEP ARCHIVE
! For archival data that rarely (if ever) needs to be accessed. Hrs/days for retrieval.
  - i.e. legal/regulation data storage
- a lot cheaper than S3 glacier flexible retrieval
- "frozen objects"
- objs retrieve to S3 Standard-IA
- retrieval options (much longer than flex retrieval):
  - standard (12hrs)
  - bulk (up to 48hrs)
- 40 KB min billing size
- 180 day min billable duration

S3 INTELLIGENT-TIERING
! for long-lived data w/changing or unknown access patterns
- automated tiering done by S3. moves objs to lower tiers if not accessed in a time range
- tiers:
  - freq access (similar to standard)
    - move to below if 30 days no access
  - infreq accss (similar to standard-IA)
    - move to below if 90 days no access
    - have to configure a prefix or tag in the bucket to be able to move lower
  - archive instance access (similar to glacier instant)
    - 90 days no access
  - archive access (similar to glacier flex retrieve)
    - 90 - 270 days no access
    - optional tier
  - deep archive (similar to glacier deep arch.)
    - 180 - 730 days no access
    - optional tier
- costs:
  - each tier is same as their similar/equivalent storage class
  - + monitoring & automation cost per 1k objs
  - no retrieval fees - the above monitoring/autom.n fee covers it

