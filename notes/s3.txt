---
S3 SECURITY

! MISC NOTE: limited to 100 buckets in an AWS acct by default

s3 is private by default

bucket policy = type of resource policy 
resource policy = like an IAM identity policy, but attached to resources
- (inverse of each other)
- identity policies define what that identity can access
- resource policies define who can access that resource

only 1 bucket policy per bucket (but can have many policy statements)

huge benefit of resource policies:
- you can allow identities from ANY account to access the resource
- note: you can't create identity policies for IAM users outside of an account.
  - so, you CAN'T grant users outside the account access to an S3 bucket thru ID policies
- resource policies can specify any user in ANY account and grant/deny access to them
- can grant anonymous access (don't know who the user/principal is) 
  - IAM policies can't do this - you can't attach an ID policy to 'nothing'

bucket policies have a "Principal" key in the Statement
- the principal could be IAM users, AWS accounts, anonymous (unauth.nd to AWS), *, etc.
- ID policies don't have a Principal property bc it's implied that the principal is the identity using that policy

can block IP addresses in resource/bucket policies

remember: policies get merged.
- if IAM user trying to access bucket, that user's policy + the bucket policy apply
  - (same for IAM users outside the bucket acct - the user's + bucket's policy apply)
- anonymous principles are only restricted by the bucket policy
  - (bc logically there is no identity policy for anonymous principles)

ACLs
- legacy - AWS suggests to use bucket policies/ID policies instead
  - ACLs are inflexible & have simple perm.s only
  - can kind of ignore they exist
  - never use ACLs unless you need to
- ACLs are a subresource of a bucket

Block Public Access feature
- added to S3 due to PR disasters - lots of data leaks bc users didn't understand S3 security properly - lot of user misconfigurations
- another security layer in front of the resource policy
  - "block public access" apply no matter what the bucket policy says
- applies only to anonymous users, not IAM identities
- tip: if you grant public access in a bucket policy but it's not working, it's probably because "block public access" is enabled

tradeoffs between identity policies vs resource policies - choice depends on use case
- res. policies - good for base level permissions on svc

---
S3 STATIC HOSTING

allows access via HTTP (think web browsers)

must set an index and error document
- must be html docs bc that's what static hosting delivers

"website endpoint" = url to hit the bucket from the internet (DNS)
if you want a custom domain, must make bucket name same as domain name

use cases:
offloading:
- serve static images from S3 instead of serving them from a compute service (ec2)
  - this will save costs since compute is expensive - s3 isn't
  - ex: dynamic html doc retrieved from ec2 points to images in s3 bucket
out-of-band pages:
- if main server (ec2) is offline for maintenance, can route customers to s3 to get a maintenance page
  - using another ec2 instance to serve the maintenance page might be an issue, because what if ec2 itself is having issues? better to use s3 scenarios like this[

pricing:
1. storing data in s3 - GB/month
2. transferring data out of buckets - per GB charge
- never charged for transferring in to buckets
3. data requests (operations) - per 1000 requests
- concern for static hosting if lot of customers
free tier:
- 5 GB storage

---
OBJECT VERSIONING & MFA DELETE

versioning is disabled by default
! once turned on, you CAN'T turn it off
- you can suspend it, and re-enable, but can't turn off
  - suspending: "This suspends the creation of object versions for all operations but preserves any existing object versions.
! you're billed for ALL versions of ALL objs
- only way to 0 the costs is to delete the bucket & reupload to new bucket w/o vers.ng
1. disabled (-> enabled)
2. enabled (-> suspended)
3. suspended (-> enabled)

objects in s3 buckets have an "id" property
if vers.ng disabled, "id" = "null" for all objs
once enabled, objs will get an id
when modifying an obj, new copy with same key ("name") is created with a new ID, orig. copy is retained 
you can get certain versions by specifying the ID in the request
if no ID in req, the latest/current version is returned by default

DELETING W/VERSIONING
w/o ID specified:
- s3 creates a "delete marker" which basically hides all versions of the obj
  - all obj versions are still retained!
  - QUOTE FROM AWS CONSOLE:
    - "Deleting the specified objects adds delete markers to them"
    - "If you need to undo the delete action, you can delete the delete markers"
- you can delete the "delete marker", which essentially "un-deletes" the obj
with ID specified:
- s3 will actually delete the obj version with specified ID
- most recent version becomes the curr/latest version
  - if deleting a vers. other than the current vers., the curr vers. stays same (nothing happens besides the selected vers. being deleted)
- NOTE: everything is "permanent" when vers.ng is enabled
  - deleting a "delete marker" is PERMANENT

MFA DELETE
a config for the obj versioning feature
if enabled, MFA device is required to:
- delete obj versions
- change state of mfa delete (i.e. suspending or re-enabling)
HOW: when calling API, must provide MFA device's serial num + MFA code


---
S3 PERFORMANCE OPTIMIZATION

"Single PUT Upload"
- default for s3:PutObject uses single data stream to upload to s3
- (typical downloads from a browser use multiple streams)
- 1 data stream = slower than multiple data streams
- if network fails after 4.5 of 5.0 GB are uploaded, then the WHOLE upload fails
- AWS has max 5GB object upload to S3 using single PUT upload
  - ... but you shouldn't trust it for anything near 5GB...
- bad idea for unreliable connections that need to upload to s3

"Multipart Upload"
- faster
- data is broken up into multiple parts for upload
! min data size for multipart is 100MB
  - you can't use multipart upload if blob is under 100MB
- no reason not to use it if >100MB - benefits are to great to not use it
- can have 10,000 MAX parts - each range from 5MB -> 5GB (last part can be smaller than 5MB)
- each part can fail & be restarted in isolation, instead of restarting the whole thing
- means much less risk for uploading large data to s3
- parts upload in parallel. so tranfer rate is increased 

S3 TRANSFER ACCELERATION
- data enters the closest/best performing AWS edge location
- from there, it enters the AWS global network
  - AWS global network:
    - used to connect regions together
    - optimized for efficiency
    - public internet might take strange paths - AWS network takes direct paths
- benefits of transfer accel. increase as data size or distance increases
- turned off by default
- bucket name must be DNS compatible
  - no periods in bucket name
- when enabling, you get a DNS endpoint for the faster transfers
  - have to use that URL for accel.d transfers
  - resolved to an edge location closest to you

---
KEY MANAGEMENT SERVICE (KMS)

used by many AWS svcs

regional & public service (AWS public zone)

create, store, manage keys
operates with both symmetric and asymmetric keys
does encrypt & decrypt (and others...)
! keys never leave KMS - keys are meant to be locked inside

! provides FIPS 140-2 (*L2) standard
- some features have L3 compliance
- "KMS is a secure and resilient service that uses FIPS 140-2 validated hardware security modules to isolate and protect your keys"

some KMS keys are called CMKs (Customer Master Keys) - old naming convention

keys can be used by you, applications, & aws svcs

"KMS Keys" are just containers for the physical real keys
- has below fields:
  - ID
  - date
  - policy
  - desc
  - state

can generate or import keys

keys themselves are encrypted when stored in KMS
keys are decrypted before they encrypting/decrypting data

on encrypt request, need to send data + key you want to use
on decrypt request, do NOT need to specify the key to use
- the key info is encoded in the cipher text. KMS can tell which one to use

permissions:
- encrypt, decrypt, key creation, etc. are all SEPARATE permissions
  - called "role separation"
  - key administrator has diff perm.s than IAM users that use the key
    - key admins can delete the key

KMS keys can encrypt MAX 4KB of data
KMS can generate DEKs (Data Encryption Keys) using KMS keys - GenerateDataKey operation
DEKs can encrpyt/decrypt data >4KB
DEKs are linked to the KMS key that created it
- the DEK knows which key created it
KMS gives DEKs to the client - YOU or the SERVICE uses the DEK
- DEKs are NOT stored in KMS

DEK architecture
1. user requests DEK from KMS
2. DEK is created (using a KMS key) (GenerateDataKey)
3. KMS gives user the plaintext DEK and the encrypted DEK
  - DEK is encrypted using the same KMS key that generated it
4. user encrypts their data (ex. s3 object) with the plaintext DEK
5. user discards the plaintext DEK
6. user stores the encrypted DEK with the newly encrypted data
when need to decrypt data...
7. user requests for DEK decryption - send encrypted DEK to KMS
8. KMS decrypts the DEK using the same KMS key used to generate it
9. user uses decrypted DEK to decrypt the data
10. user discards the decrypted DEK

you can encrypt many objects/data with 1 DEK, or use a new DEK for each object/data

default = KMS keys are isolated in a region - CANNOT move a key to different region
- can't extract a key from the KMS service either
- KMS aliases are also regional
multi-region keys can be used (key replicated across regions)
- relatively new feature

customer owned keys:
- type 1: AWS managed - created automatically when using a svc that integrates w/KMS
- type 2: customer managed - created explicitly by customer for application/AWS svc usage

all keys can be rotated 
- off by default for cust. managed keys
- on by default for AWS managed keys (you can't turn off)
- done yearly when turned on
backing keys are kept so old data can still be decrypted

every key has a Key Policy (resource policy - like S3 bucket policies)
you must explicitly configure KMS to trust the account that the key is created in via key policy
- this trust isn't automatic, like it is with resources in other svcs
- need to do this before granting access to a key via identity policies
- kinda like the "allow all public access" feature in S3 - gotta do it twice
- ex. identity policy usually allows encrypt/decrypt for all KMS keys, while the key policy specifies what exact principals are allowed to encrypt, decrypt, create, etc.

Key Policy from demo
{
    "Id": "key-consolepolicy-3",
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Enable IAM User Permissions",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::711387134335:root"
            },
            "Action": "kms:*",
            "Resource": "*"
        },
        {
            "Sid": "Allow access for Key Administrators",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::711387134335:user/iamadmin"
            },
            "Action": [
                "kms:Create*",
                "kms:Describe*",
                "kms:Enable*",
                "kms:List*",
                "kms:Put*",
                "kms:Update*",
                "kms:Revoke*",
                "kms:Disable*",
                "kms:Get*",
                "kms:Delete*",
                "kms:TagResource",
                "kms:UntagResource",
                "kms:ScheduleKeyDeletion",
                "kms:CancelKeyDeletion",
                "kms:RotateKeyOnDemand"
            ],
            "Resource": "*"
        },
        {
            "Sid": "Allow use of the key",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::711387134335:user/iamadmin"
            },
            "Action": [
                "kms:Encrypt",
                "kms:Decrypt",
                "kms:ReEncrypt*",
                "kms:GenerateDataKey*",
                "kms:DescribeKey"
            ],
            "Resource": "*"
        },
        {
            "Sid": "Allow attachment of persistent resources",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::711387134335:user/iamadmin"
            },
            "Action": [
                "kms:CreateGrant",
                "kms:ListGrants",
                "kms:RevokeGrant"
            ],
            "Resource": "*",
            "Condition": {
                "Bool": {
                    "kms:GrantIsForAWSResource": "true"
                }
            }
        }
    ]
}

---
S3 OBJECT ENCRYPTION CSE/SSE

! buckets are not encrypted! objects are.
- define encryption at the object level
- ("bucket default encryption" is a different feature, separate)
- each obj could use diff encryption settings

S3 encryption (CSE/SSE) refers to encryption AT REST. 
(data is always encrypted in transit, i.e. HTTPS)

CSE = Client Side Encryption
  - the client encrypts the data before even uploading to S3
  - client takes all risk and control over encryption
  - you manage the key, and which key for which obj
  - not related to S3 at all, technically could do both CSE & S3 SSE
SSE = Server Side Encryption
  - client sends plaintext. S3 encrypts the plaintext data.

SSE/encryption at rest is mandatory in S3
- you can only change what type is used

2 Components to SSE
1. encryption & decryption (symmetric) processes
2. actual mgmt of keys

each type of SSE handles the above 2 components differently
your choice depends on how much you trust S3
your choice impacts permissions
multiple types of SSE
- SSE-C = SSE with Customer-Provided Keys
  - client generates/manages key, but compute for encyption/decryption is offloaded to S3
  1. client provides key and plaintext obj
  2. S3 encrypts the obj
  3. S3 makes a hash of the key
  4. S3 discards the key
  5. S3 stores the hash along with the ciphertext object
  - you trust that S3 will discard your key and not hold on to it
- SSE-S3 = SSE with AWS S3 Managed Keys
  - the default method 
  - uses AES256 (sometimes SSE-S3 is referred to as AES256)
  - S3 handles both encryption process, AND key generation/mgmt
  - unique key for each obj
  1. client just provides platintext obj
  2. S3 generates an object key
  3. S3 encrypts the obj with obj key
  4. S3 encrypts the obj key with the S3 Key
    - S3 Key is totally invisble to customers, you can't do anything to/with it.
  5. S3 discards the plaintext obj key
  6. S3 stores the encrypted obj key and ciphertext obj
  - you don't have control over keys here
    - shouldn't use this if compliance requires full control (key access, rotation, etc.)
  - no role separation
    - can't restrict full admin users from seeing objs
- SSE-KMS = SSE with KMS Keys Stored in KMS
  - you create KMS key so you control it, & own permissions on the key
    - OR, you can use an S3 default key. (can't change key policy, set rotations, etc.)
      - customer managed vs aws managed key
      - ! don't confuse this with SSE-S3
  - S3 & AWS do key mgmt, S3 does encryption process
  - S3 requests a DEK (using the KMS key) from KMS for each obj
  - follows DEK architecture (check earlier notes)
  - good option bc you have full control over the key/key mgmt
    - logging/auditing, rotation, role separation, etc.
  - can restrict full admins from seeing objects by not giving permissions to the KMS key

---
S3 BUCKET KEYS

used for improving KMS scalability (and S3 scalability in turn)

before bucket keys (if using sse-kms):
  each s3 Put causes a call to KMS to make a unique DEK (using the 1 specified KMS Key)
  - scaled poorly - 1-to-1 for each s3 upload 
  KMS calls have costs and throttling limits
  - 5.5k, 10k, or 50k per second across ALL regions 
  using a single KMS key results in a scaling limit for PUTS per second per key
after bucket keys:
  KMS gives a "bucket key" to the S3 bucket. that bucket key is used to make DEKs
  - the bucket key is TIME LIMITED - new bucket key will be used after time limit
  - bucket key is generated from the original KMS key
  this offloads a lot of the work from KMS to S3 (reduces KMS API calls)
  not retroactive - only affects objs after it's enabled on the bucket

NOTE: KMS CloudTrail events would show the bucket ARN instead of obj ARN
- b/c KMS is creating a bucket key, not a DEK for an object
fewer cloudtrail events - b/c KMS used less often, just to make bucket keys

works with S3 data replication across regions
encryption settings for the obj would also be same for replicated one
- but, if replicating plaintext obj to a bucket using bucket keys, the obj gets encrypted once replicated to the destination

---
S3 STORAGE CLASSES

each class offers different storage features/specs

S3 STANDARD
! Use S3 Standard for frequently accessed data that is important an non replaceable.
- objs get replicated across AT LEAST 3 AZs in the region
- objs can be public
- offers 9 9's of durability
- content-MD5 checksums
- CRCs (Cyclic Redundancy Checks) used to detect/fix data corruption
- billing:
  - GB/month fee for data storaged
  - $ per GB charge for transfer OUT
  - no fee for transfer IN data
  - price per 1K requests
  - no min duration, no min obj size
! if you get 200 OK from S3 API, then you know it was stored durability.

S3 Standard-IA (Infrequent Access)
! useful for long-lived data, which is important, but accessed infrequently
- same as S3 except below...
- about 1/2 cost to store data
- billing:
  - per request charge - same as s3 standard
  - data transfer out - same as s3 standard
  ! retrieval fee (in addition to transfer out fee!) - per GB data
  - min duration charge of 30 days (can store objs for 2 days, but get chg.d for 30)
  - min capacity charge of 128KB per obj
    - not useful for lots of tiny objects

S3 One Zone-IA (Infrequent Access)
! For long-lived, non-critical, replaceable data that isn't accessed frequently
- same as S3 Standard IA except below...
- data is only stored in ONE AZ
  - cheaper storaged for greater risk of AZ failure
- still 9 9's of durability (ASSUMING THE AZ DOESN'T FAIL)
  - data still replicated, but only within the one AZ

S3 Glacier - Instant (Instant Retrieval)
! used for long-lived data, accessed ~1/qtr, with very fast access
- like S3 Standard-IA, but cheaper storage, more expensive retrieval, longer min.s
  - want data very infrequently, but want it fast (millisecs) once it's time
- min duration charge of 90 days
- costs more if you need to access the data, but less if you don't

S3 Glacier - Flexible Retrieval (Formerly just S3 Glacier)
! For archival data where realtime or frequent access isn't needed. mins-hrs retrieval
- 1/6 the cost of data storage compared to S3 Standard
- same durability/redundancy/replication as S3 Standard
- "cold objects"
  - can't be made public
  - you can see them in console, but those are just ptrs to the objs
- have to do a retrieval process to get objects
  - job that S3 has to run
  - once retrieved, they're stored with S3 Standard class on temp basis
  - you pay for the retrieval process
- retrieval options:
  - expedited (1-5 mins)
  - standard (3-5 hours)
  - bulk (5-12 hours)
  - faster = more $$$
! first byte latency = mins or hours
- 40 KB min billing size
- 90 day min duration billing

S3 GLACIER - DEEP ARCHIVE
! For archival data that rarely (if ever) needs to be accessed. Hrs/days for retrieval.
  - i.e. legal/regulation data storage
- a lot cheaper than S3 glacier flexible retrieval
- "frozen objects"
- objs retrieve to S3 Standard-IA
- retrieval options (much longer than flex retrieval):
  - standard (12hrs)
  - bulk (up to 48hrs)
- 40 KB min billing size
- 180 day min billable duration

S3 INTELLIGENT-TIERING
! for long-lived data w/changing or unknown access patterns
- automated tiering done by S3. moves objs to lower tiers if not accessed in a time range
- tiers:
  - freq access (similar to standard)
    - move to below if 30 days no access
  - infreq access (similar to standard-IA)
    - move to below if 90 days no access
    - have to configure a prefix or tag in the bucket to be able to move lower
  - archive instant access (similar to glacier instant)
    - 90 days no access
  - archive access (similar to glacier flex retrieve)
    - 90 - 270 days no access
    - optional tier
  - deep archive (similar to glacier deep arch.)
    - 180 - 730 days no access
    - optional tier
- costs:
  - each tier is same as their similar/equivalent storage class
  - + monitoring & automation cost per 1k objs
  - no retrieval fees - the above monitoring/autom.n fee covers it


---
S3 LIFECYCLE CONFIGURATION

Lifcycle Configuration = a set of rules which consist of actions that apply to a bucket or groups of objects
- group of objects - can group by prefix or tags
- rule = "do x if y is true"

automates moving objs to other storage classes or deleting/cleaning up objs

Actions
can apply to versioned buckets/objs
transition actions = transition objects into a different storage class
expiration actions = delete (expire) objs/obj versions

think of a waterfall
! cannot transition upwards - only down (ex. can't go from S3 glacier to S3 standard)
flows:

! beware of moving smaller objs to lower tiers - can end up paying more due to min size $

! if put obj in S3 standard, have to wait 30 days before moving the obj to standard-IA or one zone -IA (when using lifecycle config)
  - can still do directly from CLI/UI/etc. if needed

! can't transition to standard-IA or one zone-IA then to glacier classes within 30 days
- Standard -(30d)> Standard-IA -(30d)> Glacier
- this applies to a SINGLE RULE - can still have 2 separate rules that do this under <30d

---
S3 REPLICATION

allows to configure replication btwn src. and dest. S3 bucket

CRR = Cross-Region Replication - replicate to a dest. bucket in a diff region
SRR = Same-Region Replication - replicate to a dest. bucket in the same region
both above can have dest. bucket in a diff AWS acct

how replication is done:
- have to set repl.n config on source bucket
  - enable versioning on both src and dest buckets
- set dest. bucket in config
- create a role which src bucket can assume that allows to copy from src and write to dest
  ! if dest. bucket in diff account...
    - bucket policy of dest. has to allow the role in the src. acct. to write to it
- done using SSL/TLS - encrypted in transit

Options:
- what to copy - all objs or a subset (based on prefix or tags)
- storage class - by default, dest. uses same storage class as src obj
  - can configure this to use diff class
  - ex. personal project, just need repl.n for safety, primary bucket used for reads, so dest. bucket can use standard-IA or lower
  - saves $ for the dest. bucket - even though 2x data, you can save some
- ownership - default owner is the src acct
  - fine if same-acct replication, but maybe concern for cross-acct repl.n
    - means dest. bucket/acct cannot read the objs
  - can configure owner to be the dest. acct.
- Replication Time Control (RTC) - enables a 15m SLA for replication
  - used if you need to ensure buckets are in sync within 15m for regulations/etc.
  - "Replication Time Control replicates 99.99% of new objects within 15 minutes and includes replication metrics. Additional fees will apply"

! Considerations:
- repl.n is NOT RETROACTIVE by default
  - have to use "batch replication" to replicate existing objs
- versioning needs to be turned on in order to use repl.n (for BOTH buckets)
  - can turn on when turning on repl.n
- repl.n is one-way by default (can do bi-directional repl.n if explicitly config.d only)
  - one-way - if you put an obj in the dest. bucket, it won't get repl.d to the src.
- can handle unenc.d, SSE-S3, SSE-KMS, SSE-C
  - need extra config to handle SSE-KMS repl.n
  - note: SSE-C wasn't historically compatible with repl.n - but now it is - 2023
- src bucket owner neees perm.s on the objs that get repl.d in dest. bucket
  - src bucket acct might not own some objs in dest. acct by default - need to allow src to be the owner
- doesn't repl. system event (lifecycle mgmt)
- can't replicate objs that are in glacier or glacier deep archive
- doesn't replicate deletes by default
  - if delete an obj in src., it will still exist in dest., because deletes aren't repl.d
  - can enable this though - "DeleteMarkerReplication"

Purposes:
SRR:
- syncing PROD and TEST aws accts in same region (cross acct)
- for resiliency, even though company has strict data sovereignty requirements
CRR:
- global resilience
- latency reduction - buckets closer to users will be faster
- for DR of an S3 static site

---
S3 PRESIGNED URLS

issue scenario:
- have a private s3 bucket
  - AWS authenticated users can access it (with correct policies/auth.z)
- need to give unauthenticated applications/identities access to objs in the bucket
- 3 un-ideal solutions for this:
  1. give the unauth.d identity an AWS identity
  2. provide AWS credentials to it/them
  3. make the bucket public

S3 PreSigned URLs address this above scenario

How it works:
1. an AWS IAM user (with correct perm.s) requests S3 to generate a PreSigned URL
  - the URL has info encoded into it
    - the specific bucket
    - the specific obj
    - the necessary auth.n info of the IAM user that created it
    - the perm.s allowed
    - the time limit (ex. 2/3 hrs)
2. the presigned URL is created and given to IAM user
3. IAM user passes the URL to the unauth.d user
4. user can use the URL to access the obj/bucket
- the URL is TIMED - after time expires, URL cannot be used
- when someone uses the URL, they are accessing the obj AS the IAM user that gen.d it
- URL can be used for GET and PUTs based on perm.s

presigned URLs are good for when you offload media into S3 from your app
also good for serverless architectures when s3 objs should stay private
- don't need to run thick app servers to broker access

scenario:
1. user visits web app to watch a video
2. app server gets presigned URL from S3 using the IAM User credentials it has
  - note: IAM User can be used for app servers b/c it's 1 identity needing access
3. presigned URL is sent back to the user's web app client
4. user/web app can use the presigned URL to view the video for limited time

misc:
! don't generate presigned URLs using an IAM ROLE.
  - the temporary cred.s created when assuming role will probably expire before URL expiry
    - URL expiry times are usually long, while role cred.s from STS are short
  - should alwaays use long-term idenities/cred.s (IAM User)
- technically, you could create a URL for an obj that your IAM User doesn't have access
  - only need to specify obj and time limit
  - however, this means obj cannot be accessed from the URL, since authN/authZ is baked in
  - not many use cases for this
  - note: if you later grant the IAM User that gen.d the URL access to the obj, the URL will work
- the URL perm.s match the perm.s that the IAM User has RIGHT NOW (at obj access time)
  - if you could access the obj through URL a day ago, but not now, maybe perm.s changed
- you can generate a URL for a non-existent obj
  - xml output when visiting URL: <Message>The specified key does not exist.</Message>

example of URL generation from CLI
[cloudshell-user@ip-10-136-60-225 ~]$ aws s3 presign s3://animals4lifemediabucket-ne/all5.jpg --expires-in 180
https://animals4lifemediabucket-ne.s3.us-east-1.amazonaws.com/all5.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIA2LIP2HF7SGVJHSNW%2F20241109%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241109T204616Z&X-Amz-Expires=180&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEPX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCZV63w2fhxmRC4sf93t9vHgxnndiDE10H%2Fb1YgE8I3QQIhAO3L10rNXuWpQ1yjejLR7QpvRoGYKXHtc9%2FwIMmnWMaJKo0DCH4QABoMNzExMzg3MTM0MzM1IgyBrNhSsADdmGKJMn4q6gIAMUbQCH1PqKgmiJPhlQqlg5r8bMY83Y5OjpNQYKtw80ODAEWgJ1s7NmRCpWuphdLDo4CgrqhmkZwYmKXaOuKF86WM7Ypbo2MuhrIlcZPL%2FsPOuJKUtyAWK0%2BiNbByYeWmG54A18MxWKWj4F%2BE7N2ktHfba9Hhv46vjI9omPDmeB1D3%2B5U6POiT1uYv5s23dAf4HbvMson9kt0JDlDnoOhedHc0E20TjuK9z6%2BYRP0LDIxqszwXjkbzpac%2BIH%2BgP%2BqbdTRqn3JnUdBPBH%2BehFeyGvafscjh0v25uu6uzw4Zn%2BuYKwXE9rXVx5kynhk58tfO0cbSB54iPm9U9IS%2BUdkquzIm2w%2F%2BY9xvu3T%2FU%2B90vyw0dPWEmofXUnv2ZMFpqzArw%2BTa24GxXIdtBVGQW8%2BF8o%2BDAyJSydWFZltLQSbrMsjGFkqzWMKqZJUVRR3SUgnpJS9aedgQ0ccjR0KOIz84firUi1TTLyzUTC7hb65BjqyAjULqw0igYSMM5mZCMcO1wydQUC%2F4lAhLZkMhEeqjnxOwM4PcLaa%2FXWEeBjzwo8rU2pOQdKsB4IODsS9Laou%2B%2B8%2FE0N4B7TkVpBwe9T8NDxmUn%2BKHuPb%2B6BklTsu2lpCcfA3HA0iRPj75AKHAX%2BatqWZz7UIH2fMHq18aMDfGegGascwENxOq3QROfCevq2voLQGpN4P0oFgV4Xw%2Bf5Qu8KR%2FU39%2FuGctb3xS578tT1tpEqz%2FkjvEEfYtybZ6DSJJjKKunzggtdKmivUvlXvi0RwJ85TuCtB0Ovc2N5SZCSQrch9sHT%2Bqp03NvPRFsSl3LyeGWUZh3BhtdTzjRZVUsiQ03TooXlYd%2BJg4PoKt%2Fku1YSM6rWmWF%2F%2FbyVCRX2RDraYnz0Q1wu7PeEEC7TjnSfXrw%3D%3D&X-Amz-Signature=12c3df5d48a1e1764a98a00818c20ddc50b8c3676d4f4e78234076e7ac5355eb

---
S3 SELECT AND GLACIER SELECT

can retrieve parts of objs instead of the whole obj

compatible with CSV, JSON, Parquet, BZIP2 compresssion for CSV and JSON

S3 can store huge objects (up to 5TB) - infinite number of objs
- GETting whole obj will take long time and you'll get billed a lot
- client can filter out unneeded data (i.e. do the query on client side), but whole obj is still sent from S3, you get billed for all the data transfer

can create SQL like statements, give query to the service (s3 select or glacier select) and service will return only that part of the obj
- "pre-filtered" by s3 - only that part of the obj is sent back in response
- faster and cheaper so only receive parts

S3/glacier filtering happens on S3 side
applies query to raw data

remember: client apps have to be aware and integrate with this feature

---
CROSS-ORIGIN RESOURCE SHARING (CORS)

the site the browser requests on first connection is the "origin" (ex. catagram.io)
browser sets that domain as the origin
same-origin requests are always allowed
Cross-Origin Request = requests to domains (aka origins) that are diff from defined origin
cross-origin req.s are restricted by default
- but, can be controlled/influenced by CORS configurations

CORS configs have to be defined on the other origins/domains
the config defines which origins they allow req.s from
  - can be specific domain or "*"
  - if don't specify, req will fail
(original origin always allows req.s to it)

CORS policy in S3
- defined as JSON array
- can specify multiple statements
- statements are defined in order. first matching rule is used
- note: these CORS configs are NOT permissions

Preflight requests
- just a check in advance before the main req. to the other origin
- HTTP req. to other origin to check if the real req. is safe to send
- triggered for anything more complicated than a "simple request" (ex. GET req.s)

CORS headers:
part of response to the preflight req. from other origin
- Access-Control-Allow-Origin
  - "*" or a specific origin
- Access-Control-Max-Age
  - how long the results of a preflight can be cached
  - preflight results are cached autom.y, so don't have to do on every single req.
  - this header controls how long the results are cached
- Access-Control-Allow-Methods
  - allowed methods for other origin, i.e. GET/POST/PUT/etc. or "*"
- Access-Control-Allow-Headers
  - indicates which HTTP headers can be used in the ACTUAL req. (not the preflight)

remember - original URL in the browser is defined as the origin - other domains need CORS.

---
S3 EVENTS

notifications can be generated when events occur in a bucket
these can be delivered to SNS topics, SQS queues, or Lambda functions

event types:
- object created (PUT, POST, COPY, CompleteMultiPartUpload)
- object deleted (*, Delete, DeleteMarkerCreated)
  - useful for security
- object restore from S3 glacier (restore initiated, restore complete)
  - useful to send emails when archive restore is complete
- replication (OperationMissedThreshold (15m SLA), OperationReplicatedAfterThreshold, OperationNotTracked, OperationFailedReplication)

define an event notification config on s3 bucket

each destination service (lambda/sns/sqs) need to allow the S3 principal to interact
- resource policy

the S3 events are just JSON

can use EventBridge too
- get access to more types of events
- can interact with wider range of AWS svcs
- should usually default to using EventBridge - but still know S3 events exist

---
S3 ACCESS LOGS

just logs of what was accessed in the bucket, when, etc.
logs get placed in a diff bucket
- src bucket = bucket we want to monitor
- target bucket = bucket that stores the access logs
have to enable access logging on the src bucket via console or CLI (PUT Bucket Logging)
S3 Log Delivery Group:
- reads configs of src bucket to see if logging is turned on 
- does the actual logging to the target bucket
need target bucket ACL to allow S3 Log Delivery Group perm.s
logging is done "best effort" - logs delivered within a few hours, NOT immediately

log files:
- each log is newline delimited, each attribute is space delimited (like normal logs)

can have many src buckets log to the same target bucket using prefixes
- configure this on the src buckets

helps with:
- security/access audits
- understand customer access patterns - insights
- understanding charges on S3 bill

you are responsible for the lifecycle of the logs
- i.e. deleting or moving to other storage classes

---
S3 REQUESTER PAYS

setting that makes the client's/requester of S3 objs pay for the transfer OUT charges
- costs are paid by identities requesting the objs
- remember: transfer IN to S3 is free, even for large data sets
- GB/month storage cost is always the bucket owner's responsibilty not matter what

useful for very popular data sets in S3

"requester pays" is a configuration on the bucket - NOT per obj

unauthenticated access is not supported with requester pays feature
- doesn't work with static website hosting or BitTorrent
- client has to be authenticated so AWS knows what identity/acct to bill
- requesters must add the "x-amz-request-payer" header to confirm $ responsibility

---
S3 OBJECT LOCK

group of related features

object lock can be enabled on new buckets only
- raise support ticket to enabled object lock on existing buckets

requires versioning to be enabled 
- cannot pause versioning - has to always be enabled
- technically, individual versions are locked, not just the "obj"

WORM = Write-Once-Read-Many
object lock follows the WORM model/arch.
- objs cannot be deleted, or overwritten

object lock feature manages locks in below 2 ways
1. S3 Object Lock Retention period
  - specify DAYS and YEARS that object can't be touched
  - 2 modes of retention period lock
    1. COMPLIANCE
      - !!! very strict!
      ! even root user cannot adjust the lock/settings after set
      - can't adjust/delete/overwrite objs
      - can NOT adjust the retention period after set
      ! obj has to stay there as-is until retention period is up
    2. GOVERNANCE
      - can't adjust/delete/overwrite objs...
      - BUT, special perm.s can be granted to allow adjusting lock settings
        - identity must be granted "s3:BypassGovernanceRetention" perm.s
        - "x-amz-bypass-governance-retention:true" header must be passed in req.s
          - (this is default when using the console UI)
        - allows modifying obj within the retention period
2. S3 Object Lock Legal hold
  - you don't set any retention period at all - no retention concept
  - just set an obj version as ON or OFF
  - can't delete or change an obj until hold is removed
  - s3:PutObjectLegalHold perm.s required to toggle the legal hold setting
  - use case: prevent accidental deletion of critical obj versions

- an obj can have both, either, or none of the above lock versions
- these are defined on specific obj versions
- can specify defaults for above on bucket level (like obj encryption settings)
- if use both on an obj, then the effects usually overlap (i.e. you still can't touch)

