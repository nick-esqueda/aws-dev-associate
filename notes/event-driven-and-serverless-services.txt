---
ARCHITECTURE NOTES

Monolithic Arch. = all features are bundled into 1 code base
- fail together, scale together, bill $ together

Tiered Arch. = features/concerns split into different services
- can use load balancers to abstract the tiers and allow horizontal scaling
- still tightly coupled somewhat 
  - they rely on each other 
  - rely on responses from dependencies
  - require at least one instance of dependency to exist and to work

Microservice Architecture = app split up into smaller pieces
- MSs can be viewed as: Producers, Consumers, or both
  - produce data or messages
- queues can communicate events btwn MSs

Event Driven Architecture = just a collection of event producers & consumers
- only consumes resources when required. no constant running or waiting
  - (this is also a key requirement for serverless)
- producers generate events to have an action taken on it
- consumers consume events via events

Queues:
- accepts messages
- messages can be polled off the queue
- fifo (most often)
- decouples application tiers/components when queue is placed in between.
  - allows tiers to not care if dependent component is even available/ready

"worker/fleet architecture"

scale from 0-inf based on queue length

Producers: could be... 
- componenents customers interact with (clicking a 'submit' button)
- parts of infra (ec2)
- systems monitoring components (error during packing an order)
- just anything that produces an event in reaction to something
Cosumers: could be...
- waiting for events to occur
- dislpaying something for a customer
- dispatch a human to resolve an order packing issue
- retry an upload

Both producers & consumers don't sit around waiting
they don't consume resources unless something needs to be done (event occurrs)

Event Router = central exchange point for events
- has an event bus
- adds producer's events to the event bus 
- router can deliver those events to consumers
- decides which consumer to deliver an event
Event Bus = like a constant flow of information

Event examples: clicks, errors, criteria met, uploads, actions, etc...

---
AWS LAMBDA

FaaS = Function as a Service
Function = just a piece of code lambda will run
key part of serverless/event driven architectures
"lambda function" = can be thought of as the lambda code + it's configuration

need to define which runtime the function needs (ex. python3.8)
need to define resources/memory the function can use
- vCPU is allocated indirectly (it's based on memory allocation/usage)
  - i.e. you don't get pick the vCPU allocation
entire runtime env. starts up on function invocation
functions are stateless - brand new env on each invocation
- entire new runtime env spins up on each invocation, then gets torn down once ended
runtime environment aka execution context

$$ based on duration of function per invocation (& runtime env used)

! Docker is an ANTI-PATTERN for lambda(?)
but lambda supports using Docker images - distinct from the word "Docker" though
- can build images specifically for use in lambda environment
  - i.e. images used for Docker and images used for Lambda are 2 diff things
  - they just share existing Docker build processes to build images for Lambda

common runtimes - python, ruby, java, go, C#
custom runtimes, like Rust, are possible using lambda layers

mem allocation - can allocate from 128MB to 10240MB of mem for a function
storage - 512MB (default) storage allocated as /tmp in the runtime env
- can configure up to 10240MB for storage
! still just temporary memory, doesn't live past the lambda function execution

! function timeout = 15m - can't run longer than 15m

execution role - role attached to function to allow perm.s to other svcs

Common Uses
- Serverless Applications (S3, API Gateway, Lambda together can make a serverless app)
- File Processing (s3, s3 events, lambda)
- database triggers (dynamoDB, streams, lambda)
- serverless cron (eventbridge/cwevents + lambda)
- realtime stream data processing (kinesis + lambda) 
  - invoked when data added to kinesis stream

NETWORKING - PUBLIC LAMBDA
- default: fn.s are given public networking.
- can access public AWS svcs and the public internet, assuming authN/authZ is config.d
- public networking has best performance, b/c no customer specific VPC networking is required. fn.s run on shared hardware and networking, not specific to customers
- however, this means lambda CAN'T access resources in a VPC, unless they have public IPs and allow external access

NETWORKING - PRIVATE LAMBDA
- lambda is config.d to run in a private subnet
- obeys all VPC networking rules, as if it were just another svc in the network
- can consume other svcs in the subnet, assuming access is allowed
- can't access things outside the VPC, unless config.d in the network
- can use VPC Endpoint to provide access to other AWS svcs, ex. DynamoDB, same as you'd do with other things in a VPC
- treat like anything else inside a VPC!
- technically...
  - the lambda fn. exists in the AWS Lambda Service VPC
  - ENIs get created and injected into your VPC subnets
    - 1 ENI per unique combination of SGs and subnets used by ALL your lambda functions
  - ENIs are created WHEN you configure the lambda fn. 
    - 90s setup - no invocation delay (as opposed to creating ENI on each invocation)
    - done only once, modified when updating configuration if needed
  - historically:
    - an ENI was created on EACH fn. execution. 
    - was bad for performance/fn. runtime and scalability - filled up the VPC with ENIs

SECURITY
execution role = lambda assumes an execution role. gets perm.s based on permissions policy of role
- trust policy trusts lambda, perm.s policy is used for temp. cred.s to interact with other resources
resource policy = allows AWS services or external acct.s to invoke the fn.
- can't be modified via console UI - only CLI or API (? verify)

LOGGING
logs from lambda executions go to CWLogs by default! don't need to conf
- output goes into a log group with the same name as the lambda fn.
metrics (invocation success/failure, retries, latency) - go to CloudWatch
lambda can be integrated with X-Ray for distribution tracing
CWLogs requires perm.s via execution role
- AWS has a managed policy/role specifically designed for this

WAYS TO INVOKE FUNCTIONS:
- synchronous invocation - client waits for response data for failure
  - CLI/API invoking the fn. - CLI/API waits for a response from fn.
  - used if lambda is used by API Gateway - client waits for response
  - errors or retries have to be handled within the client!
- asynchronous invocation
  - typically used when AWS svcs invoke lambda fn.s
  - ex. S3 bucket with S3 Events enabled
    - S3 upload causes event to trigger lambda - S3 doesn't wait for response
  - lambda handles retries on errors (can configure between 0-2 retries)
    ! fn. must be idempotent - re-running fn. should cause the same output every time
    ! you'll get weird results if you don't make the fn. idempotent!
  - after all automatic retries, can send event to dead letter queues for processing
  - can send successful/failed events to "destinations" - SQS, SNS, Lambda, EventBridge
- event source mappings
  - an Event Source Mapping polls a source for events, and sends event batches to fn.
  - typically used on streams or queues that don't support event generation to invoke lambda (ex. kinesis, dynamoDB streams, SQS)
  - ex. pulling a batch from Kinesis Data Stream and sending event batches to fn.s 
  - the lambda execution role needs perm.s to the data source (ex. kinesis)
    - b/c the Event Source Mapping uses the execution role on lambda's behalf to retrieve data
  - failed event batches can go to dead letter queue - SQS queues or SNS topics

LAMBDA VERSIONS
- can define versions of lambda fn.s - v1, v2, v3
- version is the code + fn. configuration (runtime env, etc.)
- versions are immutable - once change is published, can't change it
  - each version has an ARN pointing to it!
- $Latest - points to the latest fn. version
- aliases - points at a version. like git tags
  - ex. DEV, STAGE, PROD

LAMBDA STARTUP
- cold start = the entire execution context has to be spun up + execute code
  - have to have cold start if no recent events
- warm start = recently run execution context is reused to run the new event
  - speeds up the fn. execution a lot since runtime is already there
- lambda invocations can reuse execution contexts
  - if used infrequently though, the context will be removed. next has to be cold start
  ! each fn. should assume it can't reuse the context though - stateless
  ! concurrent executions create new contexts - 20 simult.s req.s = 20 cold starts
  - provisioned concurrency = can tell AWS to keep X contexts warm & ready to use
    - improved start speeds, maybe if deploying a PROD release and want pre-made envs
- can use the /tmp dir like a cache for new invocations
  - can store things that other invocations can use, if the exec. context is reused
  ! fn. should still assume that the data won't be in /tmp though. just a psuedo-cache


---
EVENTBRIDGE / CLOUDWATCH EVENTS

event bridge is replacing CWEvents - has a superset of CWEvents functionality

event example - ec2 instance termination or restart or startup

key concepts
- if X happens, or at Y time(s), do Z
- there's a default event bus for each acc.t
- all services that generate an event send the event to the bus
  - (e.g. move from start to stop state)
- Event Bus = just a stream of events. can route events to 1+ targets
- can add additional busses
- supports event destnations to 3rd parties
- Event Bridge monitors all events passing through busses it can see
- Rule = determines what happens to events - send to targets when pattern matches
  - you configure these rules - linked to a specific event bus
  - pattern matching rule = matches patterns of the events themselves 
  - schedule rules = match cron formatted times/time ranges
    - rule is executed when time matches
- events get sent to all targets in parallel (assuming you config.d >1 target in rule)
- events are in JSON format. that JSON gets sent to the target

example event:
{
  "version": "0",
  "id": "7bf73129-1428-4cd3-a780-95db273d1602",
  "detail-type": "EC2 Instance State-change Notification",
  "source": "aws.ec2",
  "account": "123456789012",
  "time": "2015-11-11T21:29:54Z",
  "region": "us-east-1",
  "resources": ["arn:aws:ec2:us-east-1:123456789012:instance/i-abcd1111"],
  "detail": {
    "instance-id": "i-abcd1111",
    "state": "pending"
  }
}

---
SIMPLE NOTIFICATION SERVICE (SNS)

HA, durable, secure, pub-sub messaging service
regionally resilient - data is replicated in the region
capable of SSE - on-disk encryption
public service - need public internet connectivity with public endpoint
- svcs in a VPC can access SNS if they have public internet access
coordaintes sending and delivery of messages
message payload can be <=256KB 

Topics = the base entity of SNS
- perm.s and configs are set on topics

Publishers/Subscribers
- publishers send messages to a certain topic, and sub.s that are subbed to the topic receive those messages
- subsribers can be - HTTP endpoints, email, SQS queues, mobile PN, text msgs, lambda
- subscribers can apply filters, so that it only recieves messages from that topic that are paricular to it's functionality

Fanout architecture
- ex. single SNS topic with multiple SQS queues as subscribers
- useful for processing the same thing in diff ways
  - ex. processing a video at diff resolutions

delivery status - can get status of delivery
  - for supported subscribers - HTTP endpoints, lambda, SQS
delivery retries - reliable delivery
topic policy - like a bucket policy that allows you to specify what entities have acess to that topic (ex. cross-account)

---
SIMPLE QUEUE SERVICE (SQS)

public service - AWS public zone, can hit public endpoints with right perm.s
2 types of queues- standard or FIFO
- FIFO = first in first out ALWAYS
- standard = best-effort FIFO - it's possible to get msgs out of order from the queue
msgs can be max 256KB
- if large data is involved, can just link to that data in the message instead
  - ex. put large obj in S3 and put link to that S3 obj in msg
SQS does regional replication & is resilient by default
queue policy - allows access from external accts (just a resource policy)

HOW IT WORKS
msgs are placed on a queue
services poll the queue to get & process the next message
- this means you can control the workload throttling from the reciever/server side
when polled, msgs are HIDDEN
- they are NOT deleted until service says to delete it once done w/processing
- msgs are hidden based on the "VisibilityTimeout"
  - if VisibilityTimeout is up and isn't deleted yet, it can re-appear on the queue
  - allows for retries
  - VisibilityTimeout is configurable - 0-12hrs, default 30s
    - can be changed per queue OR per msg - "ChangeMessageVisibility" operation
can send problem msgs to Dead-Letter Queues
- ex. if msg reappers 5 times w/o deletion, then msg goes to the DLQ
- DLQs are useful if problem msgs need to be processed in a different way
ASGs can scale based on queue length
Lambdas can be invoked when msgs appear on the queue

SNS AND SQS FANOUT ARCHITECTURE EXAMPLE
video upload and transcode to diff video sizes
components:
  - "web pool" - pool of EC2s inside an ASG
    - this ASG will scale based on incoming browser client load 
  - 1 SNS topic
  - 3 SQS queues sub.d to SNS topic
    - 1 queue for each video size (480p, 720p. 1080p)
  - 3 ASGs with EC2s in each for video processing
    - 1 ASG for each video size (480p, 720p. 1080p)
    - the EC2s in each ASG poll form their respective queue
    - each ASG can scale independently of each other, and independ of web pool
- client makes req to web pool to upload video
- web pool inst.s put the user's video in S3 (full size video)
- web pool inst. sends msg to SNS topic
  - msg has link to video in S3
- SNS pushes the msg to all 3 SQS queues
- EC2s in each ASG poll from their queue to start transcoding
  - pulls orig. video from S3 bucket
- once transcode is done, put in a diff S3 bucket
- web pool can get the diff video sizes from the S3 bucket

STANDARD VS FIFO QUEUE
standard 
  - like a multi-lane highway
  - "at-least-once" delivery
    - means it's possible for a msg to be received/proccessed twice!
    - duplicate message delivery
  - "best-effort ordering" - order is not quaranteed
  - scales virtually infinitely - can "keep adding lanes"
fifo 
  - like a 1 lane highway
  - "exactly-once" delivery
    - means msg will only appear/get processed ONCE
  - doesn't scale well - "only 1 lane"
    - 3,000 msgs per sec w/batching, up to 300 msgs per sec w/o batching
    - batching = each transaction can contain 10 msgs. 1 req = batch of msgs
  - misc: queue must have a ".fifo" suffix 

billed on "requests" (polling, not messages)
- 1 req = 1-10 msgs up to 64KB total
- less cost effective the more you poll SQS
short polling - uses 1 request, returns 0 or more msgs
- still consumes a request even if 0 returned msgs
- takes up a lot of requests to keep the queue empty (target state - process msgs asap)
long polling - get to use the waitTimeSeconds value
- if 0 msgs on queue, the request will wait "waitTimeSeconds" for a msg to arrive
  - max 20s
- you SHOULD use long polling

msgs can stay up to 14 days on queue
- supports encryption at rest using KMS, since data could stay there for a while
- encrypted in transit by default between queue and clients

SQS EXTENDED CLIENT LIBRARY
- use when handling msgs over SQS max (256KB)
- all this is just a workaround for the hard 256KB limit - uses S3 as a workaround
- the extended client library stores large payloads in S3 when sending msg to Queue
  - msgs up to 2GB in size instead of 256KB
- when sending msg to queue, SQS uploads to S3, stores link in msg
- receiving the msg: SQS loads the large payload from S3 and sends whole thing to svc
- payload in S3 is deleted when deleting msg from queue

SQS DELAY QUEUES
- any queue with the "DelaySeconds" value is a Delay Queue
- DelaySeconds min=0, max=15m
- when msg is added to queue, it will be HIDDEN for DelaySeconds duration
  - when doing ReceiveMessage, 0 msgs will be returned, since the msg is hidden
- (audio example) like the attack time on an envelope
- useful if want to wait a few seconds after customer does an action for some reason
- Message timer = per-message invisibility - overrides the delay queue setting
  - ! message timers are not supported on fifo queues

DEAD-LETTER QUEUES
- aims to address problem where msg stays on a queue infinitely (ex. poll, processing fails, visibility timeout up, poll, processsing fails, vis. timeout up, ...)
- ReceiveCount value tracks how many times a msg was received before deletion
  - will increment when msg re-appears after Visibility Timeout and is polled again
- Redrive policy = defines source queue, DLQ, and conditions for msgs to move to DLQ
  - when ReceiveCount > maxReceiveCount && msg isn't deleted, msg will move to DLQ
! retention period still applies to DLQs
  ! enqueue timestamp is when msg was added to original queue, NOT the DLQ
  - retention period of DLQs are typically set longer than source queues
- multiple source queues can send to one DLQ

---
AWS STEP FUNCTIONS

addresses some limitations/design decisions of Lambda
the base entity of Step Functions are "state machines"
- can have multiple state machines
state machine = long-running serverless workflows
serverless workflow ex. - START -> STATE1 -> STATE2 -> ... -> END
states are "THINGS which occur" inside a workflow
maximum duration of a state machine = 1 YEAR
- long max duration b/c used for complex workflows that may involve human intervention
2 types of workflows
- Standard Workflow - default for most use cases - 1 YEAR max duration
- Express Workflow
machine can be started via APIGW, EventBridge, Lambda, manually, etc.
uses Amazon States Language (ASL) - JSON template to create a state machine
use IAM roles for perms, so state machine can access AWS resources

STATES
state machines just coordinate the work occurring
states control flow through the state machines
TASK states coordinate with other services to perform the actual work
can think of states like control flow in code

states:
- SUCCEED
- FAIL
- WAIT - wait for certain duration or date/time before proceeding
- CHOICE - like if/else based on input data
- PARALLEL - branch out and execute >1 things at the same time
- MAP - do something for each item in and input list
- TASK - where the actual work is done
  - integrate with AWS svcs


---
API GATEWAY

managed API gateway service
highly available & scalable
create and manage APIs on APIGW
single entrypoint for applications
sits between client apps and backend svcs (aka "integrations" - AWS svcs)
features:
- authZ
- throttling (aka rate-limiting)
- caching
- CORS handling
- transformations
- OpenAPI spec to define APIs
- direct integration with AWS svcs
can connect to svcs/endpoints in AWS, or on-prem, anywhere
supports HTTP APIs, REST APIs, WebSocket APIs
useful if migrating backend services from on-prem to cloud (ex.) 
- API stays same, and can throttle/switch requests to new/migrated backend svc

request steps - authorize -> validate -> transform
response steps - return <- prepare <- transform

API = a bundle of API endpoints (think APIM API or APIGEE proxy)
Resource = like a path inside the API (ex. /api/v1/something)
Method = the HTTP method + backend integration settings
  - get/post/put/etc.


CWLogs can store and manage req/rsp logs
CW can store metrics for client & backend logs

API Cache can be used to make rsp times faster for clients + reduce req.s to backend

AUTHENTICATION
Cognito:
- client authenticates with Cognito, receives a token, and sends token in req to APIGW
  - APIGW verifies the token by talking with Cognito (this is native functionality)
  - req proceedes to backend integration
Custom auth:
- can use a custom lambda "authorizer" to do auth on req.s
  - client sends bearer token in req to APIGW
  - APIGW calls lambda authorizer
  - lambda authorizer can make call to an identity provider or do custom compute for auth
  - lambda authorizer returns IAM policy and principal identifier
  - req proceedes to backend integration
- can also config to use no auth

ENDPOINT TYPES
- Edge-optimized - req.s get routed to the nearest CloudFront Point of Presence (POP)
  - uses CloudFront's global network - fast
- Regional - scalable & highly available within region
- Private - endpoint only accessible within a VPC via interface endpoint - totally private

STAGES
APIs in APIGW are deployed to "stages" - each stage has 1 deployment
can have diff stage for PROD and DEV
has diff endpoints - "api.catagram.io/prod" vs "api.catagram.io/dev" 
stage example: v1 of an API might be deployed to PROD stage, while v2 is in DEV stage
can rollback stages to old deployments
canary deployments:
- each stage has a canary (if enabled) - allows you to throttle new API versions
- once deploying to stage, it will deploy to the canary
- certain % of traffic will go to canary (you config the %)
  - can adjust the % over time, then promote the canary to be the new "base stage"
- can always rollback to base stage if canary is showing issues

MISC STATUS CODES 
- 429 Too Many Requests = exceeded throttle - APIGW can "throttle" req.s (aka rate limit)
- 502 Bad Gateway = bad output returned by backend integration
- 503 Service Unavailable - maybe backend endpoint is offline/can't connect
- 504 Integration Failure/Timeout - backend rsp time is above >29s APIGW limit
  ! even though lambda max run time is 15m, if it's behind an APIGW, still 29s limit

CACHING
config.d per stage
reduces load & !cost! on backend svcs, improves performance
calls are only made to backends on cache miss
! cache can be encrypted
overall cache size - 500MB up to 237GB
TTL:
- default 300s
- min 0, max 3600s


